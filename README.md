## Хакатон
[ссылка](https://alfabank.ru/alfastudents/event/hack/)

трек: склонность физических лиц к инвестициям

## Система
- оперативная память: 48G
- python: 3.10.11

# `Отборочный этап`
## Задача
Классифицировать объекты как склонных или не склонных к инвестициям физических лиц. 

## Предобработка данных: 
мы ограничились нормализацией id в некоторых моделях, отбрасыванием части фичей датасета. Прочие испробованные методы не приносили существенной пользы. Размер датасета составлял около 20 гб, что накладывало серьёзные ограничения на любую с ним работу. Но его сжатие различными алгоритмами мы быстро отбросили, в виду ухудшения качества информации, приводящего к падению метрик.

## Решение:
для сортировки фичей по важности используется встроенный функционал catboost - модели, обученной на 501(+id) параметрах.

HistGradientBoostingClassifier из sklearn, обученный на 501 фичах - первая наша модель, преодолевшая порог roc-auc 0.85. 
Были попытки с нейронными сетями на pytorch, tensorflow, и они имели определённый успех, однако выше 0.855 их roc-auc мы поднять не смогли, так что отказались от них в дальнейшем.

catboost и lightautoml(далее lama) - показали наилучшие результаты, мы сосредоточились на подборе оптимальных для них параметров и достигли некоторого успеха.
в финальном решении используются предсказания 9 моделей, из которых 6 - catboost, 3 - lama. Такая конфигурация показала себя наилучшей. [обучение этих моделей ](https://github.com/ArgentumX/CloudWalkers-solution/blob/main/%20qualifying/Models.ipynb)

Здесь собраны модели, с разным количеством отброшенных фич(0, 300, 350, 400), что скорее всего является основной причиной высокой метрики при их дальнейшем блендинге. 
Можно предположить, что при разном количестве фич модели выделяют закономерности на разных слоях, то есть, к примеру в (отсортировано по важности) фичах 200-300 содержится некоторая закономерность, связанная с результатом, а в фичах 100-200 есть своя закономерность.
При обучении на всех фичах, модель может не выделить что-то важное.

далее мы производим самый простой (усреднённое значение) блендинг предсказаний и получаем финальное предсказание. [блендинг](https://github.com/ArgentumX/CloudWalkers-solution/blob/main/%20qualifying/Blending.ipynb)

Вероятно, метрику можно увеличить ещё больше, если правильно изменить коэффиценты при блендинге, но наши попытки их автоматического поиска не принесли положительных результатов (в том числе из-за нехватки времени), так что мы остановились на усреднение предссказаний.

P.S 1: В финальном этапе мы получили в своё распоряжение правильные ответы для тестового датасета отборочного этапа. С помощью него, перебрав комбинации предсказаний, мы определили что было причиной успеха блендинга 3 lama и 6 catboost моделей (с около 0.8607 рост до 0.8618 roc-auc) - основной скачок происходит в момент блендинга 1 lama с дропом 350 незначащих фич и 1 catboost с дропом 400 фич (метрика около 0.8612). Далее по мере прибавления новых предсказаний в комбинацию она способна преодолеть порог в 0.8621. При максимальном скоре более половины моделей составляют lama (3) и остальные это catboost с дропом 400 фич и catboost, обученный на всём датасете с огромным количеством итераций. Это очень интересный результат, не до конца понятно в чём причина такого успешного объединения двух моделей на базе catboost, но вероятно дело в разнице их изначальных конфигураций у нас и в lama.

![0](https://github.com/user-attachments/assets/3844e949-27bf-43e7-9d17-703d640938f5)

# `Финальный этап`
## Задача
Создать automl решение, которое будет применено к 9 разным датасетам (4 для публичного скора и 6 для приватного). Ограничение по времени для работы алгоритма 4 часа на сразу все 9 моделей
## Предобработка данных: 
Доступа к датасетам не предоставляется. Мы пробовали всеобщую нормализацию, создание бинов для id столбца, но отбросили их в виду неэффективности.

## Решение:
В этот раз мы решили сосредоточить своё внимание на причине успеха нашего решения на отборочном этапе, для чего были проведены некоторые исследования: в первую очередь мы доработали идею о дропах различного количества фичей датасета и блендига моделей обученных на этом градиенте. Название данного метода - сумрачное обучение (Лукьяненко, Дозоры). Скорость обучения catboost моделей была идеальной для наших жёстких ограничений.  

Далее мы выяснили что играло главную роль в блендинге отборочного этапа (P.S 1). После чего мы попробовали интегрировать lama в решение, чтобы повторить успех, но он не повторился, хотя условия были очень похожи, за исключение времени обучения lama моделей. В отборочном мы могли позволить себе по 6 часов обучения одной модели, но здесь 20 минут на датасет это предел.

Далее наше решение свелось к подбору нужного "сумрачного фактора" - набора процентов отброшенных фичей датасета, на которых мы будем обучать модели. 

Стоит отметить что все испробованные нами методы ансамблирования (лог регрессия, стекинг, параметры при блендинге, блендинг sklearn) проигрывали простому усреднению, что не до конца понятно. 

P.S 2: В финале в топ 6 попали команды, использующие чистую lama и lama в ансамбле с каким-то другим automl решением, что наводит на мысли о возможности её интеграции в наше финальное решение, к сожалению, не реализованной.

![1](https://github.com/user-attachments/assets/bd434c57-3c69-4722-9488-4f5dc7ad0301)
